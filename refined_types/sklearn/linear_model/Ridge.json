{
    "sklearn.linear_model.Ridge": {
        "solver": {
            "type":{
                "kind":"EnumType",
                "types":[
                    {
                        "kind":"EnumType",
                        "name":"auto"
                    },
                    {
                        "kind":"EnumType",
                        "name":"svd"
                    },
                    {
                        "kind":"EnumType",
                        "name":"cholesky"
                    },
                    {
                        "kind":"EnumType",
                        "name":"lsqr"
                    },
                    {
                        "kind":"EnumType",
                        "name":"sparse_cg"
                    },
                    {
                        "kind":"EnumType",
                        "name":"sag"
                    },
                     {
                        "kind":"EnumType",
                        "name":"saga"
                    },
                    {
                        "kind":"EnumType",
                        "name":"lbfgs"
                    }
                ]
            },
            "docstring":{
                "type":"{'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'}, default='auto'",
                "description":"Solver to use in the computational routines:\n\n‘auto’ chooses the solver automatically based on the type of data.\n\n‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’.\n\n‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n\n‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter).\n\n‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure.\n\n‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.\n\n‘lbfgs’ uses L-BFGS-B algorithm implemented in scipy.optimize.minimize. It can be used only when positive is True.\n\nAll last six solvers support both dense and sparse data. However, only ‘sag’, ‘sparse_cg’, and ‘lbfgs’ support sparse input when fit_intercept is True."
            }
        }
    }
}
